{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4a4c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env 파일 로드 완료: ../.env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"../\")\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import os\n",
    "from typing import List, TypedDict, Literal, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = project_root / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\".env 파일 로드 완료: {env_path}\")\n",
    "else:\n",
    "    print(f\".env 파일을 찾을 수 없습니다: {env_path}\")\n",
    "\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tools import StructuredTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import Annotated\n",
    "\n",
    "from src.utils import load_vectorstore, load_llm\n",
    "from src.storage import Storage\n",
    "\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    TAVILY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TAVILY_AVAILABLE = False\n",
    "    print(\"tavily-python이 설치되지 않았습니다. 웹 검색 기능을 사용하려면 설치해주세요: pip install tavily-python\")\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    YFINANCE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YFINANCE_AVAILABLE = False\n",
    "    print(\"yfinance가 설치되지 않았습니다. 주식 기능을 사용하려면 설치해주세요: pip install yfinance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056dbca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/study/learning_langchain/weekly_project/../src/utils.py:30: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소 로드 완료\n",
      "\n",
      "Retriever 초기화 중...\n",
      "Retriever 초기화 완료\n",
      "\n",
      "Storage 초기화 중...\n",
      "Storage 초기화 완료\n",
      "\n",
      "LLM 초기화 중...\n",
      "LLM 초기화 완료\n",
      "\n",
      "Tavily 클라이언트 초기화 완료\n",
      "\n",
      "OpenWeatherMap API 키 설정 완료\n"
     ]
    }
   ],
   "source": [
    "db_path = \"../data/db\"\n",
    "storage_path = \"../data/processed\"\n",
    "\n",
    "print(\"벡터 저장소 로드 중...\")\n",
    "vectorstore = load_vectorstore(db_path)\n",
    "print(\"벡터 저장소 로드 완료\")\n",
    "\n",
    "print(\"\\nRetriever 초기화 중...\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"Retriever 초기화 완료\")\n",
    "\n",
    "print(\"\\nStorage 초기화 중...\")\n",
    "storage = Storage(storage_path)\n",
    "print(\"Storage 초기화 완료\")\n",
    "\n",
    "print(\"\\nLLM 초기화 중...\")\n",
    "llm = load_llm()\n",
    "if llm:\n",
    "    print(\"LLM 초기화 완료\")\n",
    "else:\n",
    "    print(\"LLM 초기화 실패 (GPT_API_KEY 확인 필요)\")\n",
    "\n",
    "tavily_client = None\n",
    "if TAVILY_AVAILABLE:\n",
    "    tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    if tavily_api_key:\n",
    "        tavily_client = TavilyClient(api_key=tavily_api_key)\n",
    "        print(\"\\nTavily 클라이언트 초기화 완료\")\n",
    "    else:\n",
    "        print(\"\\nTAVILY_API_KEY가 설정되지 않았습니다. 웹 검색 기능이 제한됩니다.\")\n",
    "\n",
    "openweather_api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "if openweather_api_key:\n",
    "    print(\"\\nOpenWeatherMap API 키 설정 완료\")\n",
    "else:\n",
    "    print(\"\\nOPENWEATHER_API_KEY가 설정되지 않았습니다. 날씨 기능이 제한됩니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10920461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    query: str\n",
    "    intent: str\n",
    "    context: str\n",
    "    iteration_count: int\n",
    "    max_iterations: int\n",
    "    final_answer: str\n",
    "    intent_history: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6325444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query: str, k: int = 5) -> List[Document]:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    return retriever.invoke(query)\n",
    "\n",
    "def load_original_content(doc: Document) -> str:\n",
    "    doc_type = doc.metadata.get('type', 'text')\n",
    "    \n",
    "    if doc_type == 'table':\n",
    "        table_id = doc.metadata.get('table_id')\n",
    "        if table_id:\n",
    "            table_data = storage.get_table(table_id)\n",
    "            if table_data:\n",
    "                markdown = table_data.get('markdown', '')\n",
    "                description = table_data.get('description', '')\n",
    "                return f\"{description}\\n{markdown}\" if description else markdown\n",
    "    else:\n",
    "        chunk_id = doc.metadata.get('chunk_id')\n",
    "        text_id = doc.metadata.get('text_id') or f'text_{chunk_id}' if chunk_id is not None else None\n",
    "        \n",
    "        if text_id:\n",
    "            text_data = storage.get_text(text_id)\n",
    "            if text_data:\n",
    "                return text_data.get('original_text', doc.page_content)\n",
    "    \n",
    "    return doc.page_content\n",
    "\n",
    "def build_context(docs: List[Document]) -> str:\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        original_content = load_original_content(doc)\n",
    "        doc_type = doc.metadata.get('type', 'text')\n",
    "        \n",
    "        if doc_type == 'table':\n",
    "            table_id = doc.metadata.get('table_id', '')\n",
    "            context_parts.append(f\"[테이블 {i}: {table_id}]\\n{original_content}\")\n",
    "        else:\n",
    "            context_parts.append(f\"[텍스트 {i}]\\n{original_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6092f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculatorInput(BaseModel):\n",
    "    expression: str = Field(description=\"계산할 수식 (예: 14598 - 5653)\")\n",
    "\n",
    "class WebSearchInput(BaseModel):\n",
    "    query: str = Field(description=\"검색할 쿼리\")\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    city: str = Field(description=\"날씨를 조회할 도시 이름\")\n",
    "\n",
    "class StockPriceInput(BaseModel):\n",
    "    symbol: str = Field(description=\"주식 심볼 (예: TSLA, AAPL, 005930)\")\n",
    "\n",
    "class DocumentSearchInput(BaseModel):\n",
    "    query: str = Field(description=\"문서에서 검색할 쿼리\")\n",
    "\n",
    "class GraphVisualizationInput(BaseModel):\n",
    "    data: Dict[str, Any] = Field(description=\"그래프 데이터 (x, y 리스트 포함)\")\n",
    "    chart_type: str = Field(default=\"line\", description=\"차트 타입 (line, bar, scatter)\")\n",
    "\n",
    "def calculator_func(expression: str) -> str:\n",
    "    try:\n",
    "        allowed_chars = set('0123456789+-*/()., ')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return f\"오류: 허용되지 않은 문자가 포함되어 있습니다.\"\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"오류: {str(e)}\"\n",
    "\n",
    "def web_search_func(query: str) -> str:\n",
    "    if not tavily_client:\n",
    "        return \"오류: Tavily 클라이언트가 초기화되지 않았습니다.\"\n",
    "    \n",
    "    try:\n",
    "        response = tavily_client.search(query=query, max_results=5)\n",
    "        results = []\n",
    "        for item in response.get('results', []):\n",
    "            results.append(f\"제목: {item.get('title', '')}\\nURL: {item.get('url', '')}\\n내용: {item.get('content', '')}\")\n",
    "        return \"\\n\\n\".join(results) if results else \"검색 결과가 없습니다.\"\n",
    "    except Exception as e:\n",
    "        return f\"오류: {str(e)}\"\n",
    "\n",
    "def weather_func(city: str) -> str:\n",
    "    api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "    if not api_key:\n",
    "        return \"오류: OpenWeatherMap API 키가 설정되지 않았습니다.\"\n",
    "    \n",
    "    try:\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather\"\n",
    "        params = {\n",
    "            \"q\": city,\n",
    "            \"appid\": api_key,\n",
    "            \"units\": \"metric\",\n",
    "            \"lang\": \"kr\"\n",
    "        }\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        result = f\"\"\"도시: {data.get('name', city)}\n",
    "온도: {data.get('main', {}).get('temp')}°C\n",
    "설명: {data.get('weather', [{}])[0].get('description', '')}\n",
    "습도: {data.get('main', {}).get('humidity')}%\n",
    "풍속: {data.get('wind', {}).get('speed')} m/s\"\"\"\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"오류: {str(e)}\"\n",
    "\n",
    "def stock_price_func(symbol: str) -> str:\n",
    "    if not YFINANCE_AVAILABLE:\n",
    "        return \"오류: yfinance가 설치되지 않았습니다.\"\n",
    "    \n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        info = ticker.info\n",
    "        \n",
    "        current_price = info.get('currentPrice') or info.get('regularMarketPrice')\n",
    "        previous_close = info.get('previousClose')\n",
    "        change = current_price - previous_close if current_price and previous_close else None\n",
    "        \n",
    "        result = f\"\"\"심볼: {symbol.upper()}\n",
    "회사명: {info.get('longName') or info.get('shortName', '')}\n",
    "현재가(오늘): {current_price}\n",
    "전일 종가(어제): {previous_close}\n",
    "변동액: {change}\n",
    "시가총액: {info.get('marketCap')}\n",
    "거래량: {info.get('volume')}\n",
    "통화: {info.get('currency', 'USD')}\n",
    "\n",
    "참고: 변화율 계산이 필요하면 calculator 도구를 사용하여 \"(현재가 - 전일종가) / 전일종가 * 100\" 형식으로 계산하세요.\"\"\"\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"오류: {str(e)}\"\n",
    "\n",
    "def document_search_func(query: str) -> str:\n",
    "    docs = retrieve_documents(query, k=5)\n",
    "    context = build_context(docs)\n",
    "    return context\n",
    "\n",
    "def graph_visualization_func(data: Dict[str, Any], chart_type: str = \"line\") -> str:\n",
    "    try:\n",
    "        if \"x\" not in data or \"y\" not in data:\n",
    "            return \"오류: 데이터에 'x'와 'y' 키가 필요합니다.\"\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        if chart_type == \"line\":\n",
    "            plt.plot(data[\"x\"], data[\"y\"], marker='o')\n",
    "        elif chart_type == \"bar\":\n",
    "            plt.bar(data[\"x\"], data[\"y\"])\n",
    "        elif chart_type == \"scatter\":\n",
    "            plt.scatter(data[\"x\"], data[\"y\"])\n",
    "        else:\n",
    "            plt.plot(data[\"x\"], data[\"y\"], marker='o')\n",
    "        \n",
    "        if \"title\" in data:\n",
    "            plt.title(data[\"title\"])\n",
    "        if \"xlabel\" in data:\n",
    "            plt.xlabel(data[\"xlabel\"])\n",
    "        if \"ylabel\" in data:\n",
    "            plt.ylabel(data[\"ylabel\"])\n",
    "        \n",
    "        plt.grid(True)\n",
    "        \n",
    "        output_dir = Path(\"../output\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        filename = f\"chart_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "        filepath = output_dir / filename\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "        \n",
    "        return f\"그래프가 생성되었습니다: {filepath}\"\n",
    "    except Exception as e:\n",
    "        return f\"오류: {str(e)}\"\n",
    "\n",
    "calculator = StructuredTool.from_function(\n",
    "    func=calculator_func,\n",
    "    name=\"calculator\",\n",
    "    description=\"수식 계산 도구. 수식을 입력하면 결과를 반환합니다. 주가 변화율, 차이, 비율 등을 계산할 때 사용하세요. 예: '14598 - 5653', '(245.50 - 240.00) / 240.00 * 100' (변화율 계산)\",\n",
    "    args_schema=CalculatorInput\n",
    ")\n",
    "\n",
    "web_search = StructuredTool.from_function(\n",
    "    func=web_search_func,\n",
    "    name=\"web_search\",\n",
    "    description=\"웹 검색 도구. 주식 뉴스, 일반 정보 등을 검색합니다.\",\n",
    "    args_schema=WebSearchInput\n",
    ")\n",
    "\n",
    "weather = StructuredTool.from_function(\n",
    "    func=weather_func,\n",
    "    name=\"weather\",\n",
    "    description=\"날씨 정보 조회 도구. 도시 이름을 입력하면 현재 날씨를 반환합니다.\",\n",
    "    args_schema=WeatherInput\n",
    ")\n",
    "\n",
    "stock_price = StructuredTool.from_function(\n",
    "    func=stock_price_func,\n",
    "    name=\"stock_price\",\n",
    "    description=\"주식 현재 주가 조회 도구. 주식 심볼(예: TSLA, AAPL, 005930)을 입력하면 현재 주가, 전일 종가 등을 반환합니다. 주가 관련 질문(현재 주가, 주가 변화, 주가 비교 등)이면 반드시 이 도구를 사용하세요. 변화율 계산이 필요한 경우 calculator 도구를 추가로 사용하세요.\",\n",
    "    args_schema=StockPriceInput\n",
    ")\n",
    "\n",
    "document_search = StructuredTool.from_function(\n",
    "    func=document_search_func,\n",
    "    name=\"document_search\",\n",
    "    description=\"문서 검색 도구. 벡터 저장소에서 관련 문서를 검색합니다. 주의: 주가, 주식 가격, 주가 변화 등 주가 관련 질문은 이 도구를 사용하지 말고 stock_price 도구를 사용하세요. 문서에서 정보를 찾을 수 없거나 부족한 경우, stock_price(주가 정보)나 web_search(웹 검색) 도구를 추가로 사용하세요.\",\n",
    "    args_schema=DocumentSearchInput\n",
    ")\n",
    "\n",
    "graph_visualization = StructuredTool.from_function(\n",
    "    func=graph_visualization_func,\n",
    "    name=\"graph_visualization\",\n",
    "    description=\"그래프 시각화 도구. 데이터를 입력하면 차트를 생성합니다. data는 x, y 리스트를 포함한 딕셔너리여야 합니다.\",\n",
    "    args_schema=GraphVisualizationInput\n",
    ")\n",
    "\n",
    "tools = [calculator, web_search, weather, stock_price, document_search, graph_visualization]\n",
    "\n",
    "try:\n",
    "    if 'llm' in globals() and llm:\n",
    "        model_with_tools = llm.bind_tools(tools)\n",
    "    else:\n",
    "        model_with_tools = None\n",
    "        print(\"경고: llm이 정의되지 않았습니다.\")\n",
    "except NameError:\n",
    "    model_with_tools = None\n",
    "    print(\"경고: llm이 정의되지 않았습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb11f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intent(state: AgentState) -> AgentState:\n",
    "    query = state.get(\"query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # 키워드 기반 의도 분류를 먼저 수행 (우선순위 높음)\n",
    "    # 주가/재무 관련 키워드가 있으면 DOC_QA로 분류\n",
    "    doc_qa_keywords = [\n",
    "        \"주가\", \"주식\", \"가격\", \"변화\", \"상승\", \"하락\", \"증가\", \"감소\", \"변동\", \"차이\", \"비교\",\n",
    "        \"매출\", \"실적\", \"수익\", \"영업\", \"이익\", \"손익\", \"재무\", \"재무제표\", \"분기\", \"연간\",\n",
    "        \"TSLA\", \"AAPL\", \"005930\", \"테슬라\", \"애플\", \"삼성\",\n",
    "        \"미국\", \"중국\", \"지역\", \"지리\", \"국가별\"\n",
    "    ]\n",
    "    if any(keyword in query for keyword in doc_qa_keywords):\n",
    "        state[\"intent\"] = \"DOC_QA\"\n",
    "        return state\n",
    "    \n",
    "    # 날씨 관련 키워드가 있으면 SMALL_TALK로 분류\n",
    "    weather_keywords = [\"날씨\", \"기온\", \"온도\", \"비\", \"눈\", \"맑음\", \"흐림\", \"바람\", \"습도\", \"강수\"]\n",
    "    if any(keyword in query for keyword in weather_keywords):\n",
    "        state[\"intent\"] = \"SMALL_TALK\"\n",
    "        return state\n",
    "    \n",
    "    # 요약 관련 키워드가 있으면 SUMMARY로 분류\n",
    "    if any(keyword in query.lower() for keyword in [\"요약\", \"정리\", \"핵심\", \"요점\"]):\n",
    "        state[\"intent\"] = \"SUMMARY\"\n",
    "        return state\n",
    "    \n",
    "    # LLM 기반 의도 분류 (키워드가 없을 때만)\n",
    "    if not llm:\n",
    "        state[\"intent\"] = \"SMALL_TALK\"\n",
    "        return state\n",
    "    \n",
    "    prompt = f\"\"\"다음 사용자 발화를 보고, 의도를 아래 중 하나로 골라라: DOC_QA, SUMMARY, SMALL_TALK. 다른 말은 하지 말고, 태그만 출력해라.\n",
    "\n",
    "의도 설명:\n",
    "- DOC_QA: 문서나 데이터에 대한 질문 (예: \"테슬라 매출은?\", \"중국 대비 미국 차이\", \"주가 알려줘\")\n",
    "- SUMMARY: 이전 대화나 정보의 요약 요청 (예: \"요약해줘\", \"정리해줘\", \"핵심만\")\n",
    "- SMALL_TALK: 인사나 일반적인 대화 (예: \"안녕\", \"뭐야\", \"누구야\")\n",
    "\n",
    "사용자 발화: {query}\n",
    "\n",
    "의도:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        intent = response.content.strip() if hasattr(response, 'content') else str(response).strip()\n",
    "        \n",
    "        if intent not in [\"DOC_QA\", \"SUMMARY\", \"SMALL_TALK\"]:\n",
    "            # 기본값은 DOC_QA (정보 질문일 가능성이 높음)\n",
    "            intent = \"DOC_QA\"\n",
    "        \n",
    "        state[\"intent\"] = intent\n",
    "    except Exception as e:\n",
    "        print(f\"의도 분류 오류: {e}\")\n",
    "        state[\"intent\"] = \"DOC_QA\"  # 오류 시 기본값을 DOC_QA로 변경\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21182821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_node(state: AgentState) -> AgentState:\n",
    "    if not model_with_tools:\n",
    "        state[\"messages\"].append(AIMessage(content=\"LLM이 설정되지 않아 답변을 생성할 수 없습니다.\"))\n",
    "        return state\n",
    "    \n",
    "    query = state.get(\"query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # 현재 질문과 관련된 메시지만 필터링\n",
    "    # SystemMessage는 유지하고, 현재 질문의 HumanMessage부터 시작\n",
    "    # 현재 질문 이후의 모든 메시지를 순서대로 포함 (AIMessage와 ToolMessage의 쌍을 유지)\n",
    "    filtered_messages = []\n",
    "    current_query_index = -1\n",
    "    \n",
    "    # 현재 질문의 HumanMessage 인덱스 찾기\n",
    "    for i, msg in enumerate(messages):\n",
    "        if isinstance(msg, HumanMessage) and msg.content == query:\n",
    "            current_query_index = i\n",
    "            break\n",
    "    \n",
    "    if current_query_index == -1:\n",
    "        # 현재 질문이 없으면 모든 메시지 사용 (첫 질문인 경우)\n",
    "        filtered_messages = messages\n",
    "    else:\n",
    "        # SystemMessage는 항상 포함\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, SystemMessage):\n",
    "                filtered_messages.append(msg)\n",
    "        \n",
    "        # 현재 질문 이전의 모든 AIMessage를 확인하고, tool_calls가 있으면 대응하는 ToolMessage를 찾아야 함\n",
    "        # 역순으로 확인하여 가장 가까운 것부터 처리\n",
    "        aimessages_with_tool_calls = []\n",
    "        for i in range(current_query_index - 1, -1, -1):\n",
    "            msg = messages[i]\n",
    "            if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                # tool_calls가 있는 AIMessage를 찾았으면, 그 tool_call_id들을 수집\n",
    "                tool_call_ids = set()\n",
    "                for tool_call in msg.tool_calls:\n",
    "                    if isinstance(tool_call, dict):\n",
    "                        tool_call_id = tool_call.get('id') or tool_call.get('tool_call_id')\n",
    "                    else:\n",
    "                        tool_call_id = getattr(tool_call, 'id', None) or getattr(tool_call, 'tool_call_id', None)\n",
    "                    if tool_call_id:\n",
    "                        tool_call_ids.add(tool_call_id)\n",
    "                \n",
    "                if tool_call_ids:\n",
    "                    aimessages_with_tool_calls.append((i, msg, tool_call_ids))\n",
    "        \n",
    "        # tool_calls가 있는 AIMessage들을 순서대로 처리\n",
    "        for aimessage_idx, aimessage, tool_call_ids in reversed(aimessages_with_tool_calls):\n",
    "            # AIMessage 포함\n",
    "            filtered_messages.append(aimessage)\n",
    "            \n",
    "            # 대응하는 ToolMessage를 전체 messages에서 찾아서 포함\n",
    "            tool_messages_to_include = []\n",
    "            for i, msg in enumerate(messages):\n",
    "                if isinstance(msg, ToolMessage):\n",
    "                    tool_call_id = getattr(msg, 'tool_call_id', None)\n",
    "                    if tool_call_id and tool_call_id in tool_call_ids:\n",
    "                        tool_messages_to_include.append((i, msg))\n",
    "                        tool_call_ids.discard(tool_call_id)\n",
    "                        if not tool_call_ids:\n",
    "                            break\n",
    "            \n",
    "            # ToolMessage를 인덱스 순서대로 추가 (AIMessage 이후, 현재 질문 이전에 있는 것만)\n",
    "            for i, tool_msg in sorted(tool_messages_to_include):\n",
    "                if aimessage_idx < i < current_query_index:\n",
    "                    filtered_messages.append(tool_msg)\n",
    "        \n",
    "        # 현재 질문부터 끝까지 모든 메시지 포함\n",
    "        # 이렇게 하면 AIMessage의 tool_calls와 대응하는 ToolMessage가 함께 포함됨\n",
    "        for i in range(current_query_index, len(messages)):\n",
    "            msg = messages[i]\n",
    "            # SystemMessage는 이미 추가했으므로 스킵\n",
    "            if not isinstance(msg, SystemMessage):\n",
    "                filtered_messages.append(msg)\n",
    "    \n",
    "    res = model_with_tools.invoke(filtered_messages)\n",
    "    return {\"messages\": [res]}\n",
    "\n",
    "def should_continue_tools(state: AgentState) -> Literal[\"tools\", \"continue\", \"end\", \"check_document\"]:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return \"end\"\n",
    "    \n",
    "    # document_sufficient가 True면 이미 충분하다고 판단했으므로 check_document로 가지 않음\n",
    "    document_sufficient = state.get(\"document_sufficient\", False)\n",
    "    if document_sufficient:\n",
    "        last_message = messages[-1]\n",
    "        if isinstance(last_message, AIMessage):\n",
    "            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "                # document_search가 아닌 다른 도구만 체크\n",
    "                tool_names = [tc.get('name') for tc in last_message.tool_calls]\n",
    "                if 'document_search' not in tool_names:\n",
    "                    return \"tools\"\n",
    "            return \"continue\"\n",
    "        return \"end\"\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if isinstance(last_message, AIMessage):\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            tool_names = [tc.get('name') for tc in last_message.tool_calls]\n",
    "            if 'document_search' in tool_names:\n",
    "                return \"check_document\"\n",
    "            return \"tools\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "    \n",
    "    return \"end\"\n",
    "\n",
    "def check_document_result_node(state: AgentState) -> AgentState:\n",
    "    query = state.get(\"query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    intent = state.get(\"intent\", \"DOC_QA\")\n",
    "    \n",
    "    print(f\"[DEBUG] check_document_result_node: 실행 시작\")\n",
    "    print(f\"[DEBUG] check_document_result_node: query={query}, intent={intent}, messages 개수={len(messages)}\")\n",
    "    \n",
    "    if intent != \"DOC_QA\":\n",
    "        print(f\"[DEBUG] check_document_result_node: DOC_QA가 아니므로 스킵\")\n",
    "        return state\n",
    "    \n",
    "    # 이미 확인했다면 스킵 (반복 실행 방지)\n",
    "    document_checked = state.get(\"document_checked\", False)\n",
    "    if document_checked:\n",
    "        print(f\"[DEBUG] check_document_result_node: 이미 확인했으므로 스킵\")\n",
    "        return state\n",
    "    \n",
    "    document_search_result = None\n",
    "    \n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, ToolMessage) and msg.name == \"document_search\":\n",
    "            document_search_result = msg.content\n",
    "            print(f\"[DEBUG] check_document_result_node: document_search ToolMessage 결과 발견, 길이={len(document_search_result)}자\")\n",
    "            break\n",
    "    \n",
    "    if not document_search_result:\n",
    "        document_search_result = state.get(\"context\", \"\")\n",
    "        if document_search_result:\n",
    "            print(f\"[DEBUG] check_document_result_node: document_search state 결과 발견, 길이={len(document_search_result)}자\")\n",
    "    \n",
    "    if not document_search_result or document_search_result.strip() == \"문서 검색 결과가 없습니다.\":\n",
    "        stock_keywords = [\"주가\", \"주식\", \"가격\", \"변화\", \"상승\", \"하락\", \"증가\", \"감소\", \"변동\", \"차이\", \"비교\", \"저번주\", \"이번주\", \"어제\", \"오늘\", \"전일\", \"현재가\"]\n",
    "        is_stock_query = any(keyword in query for keyword in stock_keywords)\n",
    "        \n",
    "        if is_stock_query:\n",
    "            system_msg = SystemMessage(content=\"\"\"문서 검색 결과가 없습니다. 주가 관련 질문이므로 stock_price 도구를 사용하세요.\"\"\")\n",
    "        else:\n",
    "            system_msg = SystemMessage(content=\"\"\"문서 검색 결과가 없습니다. web_search 도구를 사용하여 최신 정보를 검색하세요.\"\"\")\n",
    "        \n",
    "        messages.append(system_msg)\n",
    "        state[\"messages\"] = messages\n",
    "        state[\"document_checked\"] = True  # 결과가 없어도 이미 확인했음을 표시\n",
    "        return state\n",
    "    \n",
    "    stock_keywords = [\"주가\", \"주식\", \"가격\", \"변화\", \"상승\", \"하락\", \"증가\", \"감소\", \"변동\", \"차이\", \"비교\", \"저번주\", \"이번주\", \"어제\", \"오늘\", \"전일\", \"현재가\"]\n",
    "    is_stock_query = any(keyword in query for keyword in stock_keywords)\n",
    "    \n",
    "    if is_stock_query:\n",
    "        return state\n",
    "    \n",
    "    if not llm:\n",
    "        return state\n",
    "    \n",
    "    prompt = f\"\"\"다음은 document_search 도구의 실행 결과입니다.\n",
    "\n",
    "사용자 질문: {query}\n",
    "\n",
    "문서 검색 결과:\n",
    "{document_search_result[:1500]}\n",
    "\n",
    "위 문서 검색 결과만으로 사용자 질문에 충분히 답변할 수 있는지 판단하세요.\n",
    "다음 형식으로 답변하세요:\n",
    "- 충분하면: \"충분: [이유]\"\n",
    "- 부족하면: \"부족: [이유]\"\n",
    "\n",
    "답변:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"[DEBUG] check_document_result_node: 문서 검색 결과 충분성 판단 중...\")\n",
    "        print(f\"[DEBUG] check_document_result_node: 사용자 질문: {query}\")\n",
    "        print(f\"[DEBUG] check_document_result_node: 문서 검색 결과 길이: {len(document_search_result)}자\")\n",
    "        response = llm.invoke(prompt)\n",
    "        judgment = response.content.strip() if hasattr(response, 'content') else str(response).strip()\n",
    "        print(f\"[DEBUG] check_document_result_node: LLM 판단 결과: {judgment}\")\n",
    "        \n",
    "        # 판단 이유 추출\n",
    "        if \":\" in judgment:\n",
    "            judgment_parts = judgment.split(\":\", 1)\n",
    "            judgment_result = judgment_parts[0].strip()\n",
    "            judgment_reason = judgment_parts[1].strip() if len(judgment_parts) > 1 else \"\"\n",
    "            print(f\"[DEBUG] check_document_result_node: 판단: {judgment_result}\")\n",
    "            if judgment_reason:\n",
    "                print(f\"[DEBUG] check_document_result_node: 판단 이유: {judgment_reason}\")\n",
    "        else:\n",
    "            judgment_result = judgment\n",
    "            judgment_reason = \"\"\n",
    "        \n",
    "        if \"부족\" in judgment_result or \"불충분\" in judgment_result or \"없음\" in judgment_result.lower():\n",
    "            print(f\"[DEBUG] check_document_result_node: 결과 부족 판단 -> web_search 사용 안내 추가\")\n",
    "            if judgment_reason:\n",
    "                print(f\"[DEBUG] check_document_result_node: 부족한 이유: {judgment_reason}\")\n",
    "            system_msg = SystemMessage(content=\"\"\"문서 검색 결과가 부족합니다. web_search 도구를 사용하여 최신 정보를 검색하세요.\"\"\")\n",
    "            messages.append(system_msg)\n",
    "            state[\"messages\"] = messages\n",
    "            state[\"document_sufficient\"] = False  # 부족함을 표시\n",
    "            state[\"document_checked\"] = True  # 이미 확인했음을 표시\n",
    "        else:\n",
    "            print(f\"[DEBUG] check_document_result_node: 결과 충분 판단 -> 추가 도구 불필요\")\n",
    "            if judgment_reason:\n",
    "                print(f\"[DEBUG] check_document_result_node: 충분한 이유: {judgment_reason}\")\n",
    "            state[\"document_sufficient\"] = True  # 충분함을 표시\n",
    "            state[\"document_checked\"] = True  # 이미 확인했음을 표시\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] check_document_result_node: 문서 결과 검증 오류: {e}\")\n",
    "        state[\"document_sufficient\"] = False\n",
    "        state[\"document_checked\"] = True  # 오류가 나도 이미 확인했음을 표시\n",
    "    \n",
    "    return state\n",
    "\n",
    "def doc_qa_node(state: AgentState) -> AgentState:\n",
    "    query = state.get(\"query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    intent = state.get(\"intent\", \"DOC_QA\")\n",
    "    \n",
    "    print(f\"[DEBUG] doc_qa_node: 실행 시작\")\n",
    "    print(f\"[DEBUG] doc_qa_node: query={query}, intent={intent}, messages 개수={len(messages)}\")\n",
    "    \n",
    "    # 새로운 질문이 들어오면 document_checked를 리셋\n",
    "    # 이전 질문의 document_search 결과와는 별개로 현재 질문에 대해 다시 검색해야 함\n",
    "    state[\"document_checked\"] = False\n",
    "    state[\"document_sufficient\"] = False\n",
    "    \n",
    "    # 현재 질문과 관련 없는 이전 메시지 필터링\n",
    "    # 현재 질문의 HumanMessage만 찾아서 그 이후의 메시지만 사용\n",
    "    filtered_messages = []\n",
    "    current_query_found = False\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage) and msg.content == query:\n",
    "            current_query_found = True\n",
    "            filtered_messages = []  # 현재 질문부터 시작\n",
    "        if current_query_found:\n",
    "            filtered_messages.append(msg)\n",
    "    \n",
    "    # 현재 질문이 없으면 모든 메시지 사용 (첫 질문인 경우)\n",
    "    if not current_query_found:\n",
    "        filtered_messages = messages\n",
    "    \n",
    "    # 현재 질문과 관련된 document_search만 확인\n",
    "    # context는 이전 질문의 결과일 수 있으므로 현재 질문에 대해서는 다시 검색해야 함\n",
    "    has_document_search = any(isinstance(msg, ToolMessage) and msg.name == \"document_search\" for msg in filtered_messages)\n",
    "    \n",
    "    if not has_document_search:\n",
    "        # 주가 관련 질문은 \"주가\", \"주식\", \"현재가\" 등 명확한 키워드만 체크\n",
    "        # \"비교\", \"차이\" 등은 매출, 실적 등 다른 것도 비교할 수 있으므로 제외\n",
    "        stock_keywords = [\"주가\", \"주식\", \"현재가\", \"전일 종가\", \"종가\", \"시가\"]\n",
    "        is_stock_query = any(keyword in query for keyword in stock_keywords)\n",
    "        \n",
    "        if is_stock_query:\n",
    "            print(f\"[DEBUG] doc_qa_node: 주가 관련 질문으로 판단 -> stock_price 사용 안내\")\n",
    "            system_msg = SystemMessage(content=\"\"\"주가 관련 질문입니다. stock_price 도구를 사용하여 주가 정보를 조회하세요.\"\"\")\n",
    "            filtered_messages.append(system_msg)\n",
    "            filtered_messages.append(HumanMessage(content=query))\n",
    "        else:\n",
    "            print(f\"[DEBUG] doc_qa_node: document_search 실행\")\n",
    "            print(f\"[DEBUG] doc_qa_node: 검색 쿼리: {query}\")\n",
    "            document_search_result = document_search_func(query)\n",
    "            result_length = len(document_search_result) if document_search_result else 0\n",
    "            print(f\"[DEBUG] doc_qa_node: document_search 결과 길이: {result_length}자\")\n",
    "            if document_search_result:\n",
    "                preview = document_search_result[:200].replace('\\n', ' ')\n",
    "                print(f\"[DEBUG] doc_qa_node: 결과 미리보기: {preview}...\")\n",
    "            else:\n",
    "                print(f\"[DEBUG] doc_qa_node: 결과 없음\")\n",
    "            \n",
    "            if document_search_result:\n",
    "                state[\"context\"] = document_search_result\n",
    "                system_msg = SystemMessage(content=f\"\"\"document_search 도구가 실행되었습니다. 결과는 아래와 같습니다:\n",
    "\n",
    "{document_search_result[:2000]}\n",
    "\n",
    "이 결과를 먼저 확인하고 사용하세요.\"\"\")\n",
    "                filtered_messages.append(system_msg)\n",
    "            \n",
    "            filtered_messages.append(HumanMessage(content=query))\n",
    "        \n",
    "        state[\"messages\"] = filtered_messages\n",
    "    else:\n",
    "        print(f\"[DEBUG] doc_qa_node: 이미 document_search 결과가 있음, 스킵\")\n",
    "        # 이미 결과가 있어도 필터링된 메시지만 사용\n",
    "        state[\"messages\"] = filtered_messages\n",
    "    \n",
    "    print(f\"[DEBUG] doc_qa_node: 실행 완료, messages 개수={len(state.get('messages', []))}\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f161c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_talk_node(state: AgentState) -> AgentState:\n",
    "    query = state.get(\"query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    intent_history = state.get(\"intent_history\", [])\n",
    "    \n",
    "    weather_keywords = [\"날씨\", \"기온\", \"온도\", \"비\", \"눈\", \"맑음\", \"흐림\", \"바람\", \"습도\", \"강수\"]\n",
    "    is_weather_query = any(keyword in query for keyword in weather_keywords)\n",
    "    \n",
    "    if is_weather_query and model_with_tools:\n",
    "        # 날씨 질문일 때는 이전 대화 기록을 무시하고 새로운 컨텍스트로 시작\n",
    "        # model 노드로 가서 tool을 호출하도록 메시지만 준비\n",
    "        fresh_messages = []\n",
    "        system_msg = SystemMessage(content=\"\"\"날씨 관련 질문입니다. weather 도구를 사용하여 날씨 정보를 조회하세요.\"\"\")\n",
    "        fresh_messages.append(system_msg)\n",
    "        fresh_messages.append(HumanMessage(content=query))\n",
    "        state[\"messages\"] = fresh_messages\n",
    "        return state\n",
    "    \n",
    "    if not llm:\n",
    "        answer = \"안녕하세요! 저는 기술 문서를 기반으로 질문에 답변하는 AI 어시스턴트입니다.\"\n",
    "    else:\n",
    "        # 현재 질문과 관련된 메시지만 사용 (날씨 질문이 아닌 경우)\n",
    "        filtered_messages = []\n",
    "        current_query_found = False\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, HumanMessage) and msg.content == query:\n",
    "                current_query_found = True\n",
    "                filtered_messages = []\n",
    "            if current_query_found:\n",
    "                filtered_messages.append(msg)\n",
    "        \n",
    "        if not current_query_found:\n",
    "            filtered_messages = messages\n",
    "        \n",
    "        chat_history = \"\\n\".join([f\"{'사용자' if isinstance(msg, HumanMessage) else '어시스턴트'}: {msg.content}\" for msg in filtered_messages[-5:]])\n",
    "        \n",
    "        prompt = f\"\"\"당신은 친절한 AI 어시스턴트입니다. 사용자와 자연스럽게 대화하세요.\n",
    "\n",
    "대화 기록:\n",
    "{chat_history}\n",
    "\n",
    "사용자: {query}\n",
    "어시스턴트:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            answer = response.content if hasattr(response, 'content') else str(response)\n",
    "        except Exception as e:\n",
    "            answer = f\"답변 생성 중 오류 발생: {e}\"\n",
    "    \n",
    "    filtered_messages.append(AIMessage(content=answer))\n",
    "    state[\"messages\"] = filtered_messages\n",
    "    state[\"final_answer\"] = answer\n",
    "    \n",
    "    intent_history.append(\"SMALL_TALK\")\n",
    "    state[\"intent_history\"] = intent_history\n",
    "    \n",
    "    return state\n",
    "\n",
    "def summary_node(state: AgentState) -> AgentState:\n",
    "    query = state.get(\"query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    intent_history = state.get(\"intent_history\", [])\n",
    "    \n",
    "    has_system_message = any(isinstance(msg, SystemMessage) for msg in messages)\n",
    "    \n",
    "    if not has_system_message:\n",
    "        doc_qa_conversations = []\n",
    "        all_conversations = []\n",
    "        human_count = 0\n",
    "        \n",
    "        for i, msg in enumerate(messages):\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                question = msg.content\n",
    "                if i + 1 < len(messages) and isinstance(messages[i + 1], AIMessage):\n",
    "                    answer = messages[i + 1].content\n",
    "                    all_conversations.append(f\"질문: {question}\\n답변: {answer}\")\n",
    "                    \n",
    "                    if human_count < len(intent_history) and intent_history[human_count] == \"DOC_QA\":\n",
    "                        doc_qa_conversations.append(f\"질문: {question}\\n답변: {answer}\")\n",
    "                human_count += 1\n",
    "        \n",
    "        tool_messages = []\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                tool_messages.append(f\"{msg.name}: {msg.content}\")\n",
    "        \n",
    "        system_instructions = [\"이전 대화 내용을 요약해주세요. 요약 중에 계산이 필요하거나 그래프를 그려야 하는 경우 calculator나 graph_visualization 도구를 사용할 수 있습니다.\"]\n",
    "        \n",
    "        if all_conversations:\n",
    "            content = \"\\n\\n\".join(all_conversations)\n",
    "            system_instructions.append(f\"이전 대화 내용:\\n{content}\")\n",
    "        elif doc_qa_conversations:\n",
    "            content = \"\\n\\n\".join(doc_qa_conversations)\n",
    "            system_instructions.append(f\"이전 DOC_QA 대화 내용:\\n{content}\")\n",
    "        \n",
    "        if tool_messages:\n",
    "            tool_content = \"\\n\".join(tool_messages)\n",
    "            system_instructions.append(f\"도구 실행 결과:\\n{tool_content}\")\n",
    "        \n",
    "        if not all_conversations and not doc_qa_conversations and not tool_messages:\n",
    "            system_instructions.append(\"요약할 이전 대화 내용이 없습니다.\")\n",
    "        \n",
    "        system_msg = \"\\n\\n\".join(system_instructions)\n",
    "        messages.append(SystemMessage(content=system_msg))\n",
    "        messages.append(HumanMessage(content=query))\n",
    "        state[\"messages\"] = messages\n",
    "    \n",
    "    return state\n",
    "\n",
    "def generate_answer_node(state: AgentState) -> AgentState:\n",
    "    query = state.get(\"query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    intent = state.get(\"intent\", \"DOC_QA\")\n",
    "    intent_history = state.get(\"intent_history\", [])\n",
    "    context = state.get(\"context\", \"\")\n",
    "    \n",
    "    last_message = messages[-1] if messages else None\n",
    "    \n",
    "    has_tool_messages = any(isinstance(msg, ToolMessage) for msg in messages)\n",
    "\n",
    "    if not llm:\n",
    "        answer = \"LLM이 설정되지 않아 답변을 생성할 수 없습니다.\"\n",
    "        messages.append(AIMessage(content=answer))\n",
    "        state[\"messages\"] = messages\n",
    "        state[\"final_answer\"] = answer\n",
    "        return state\n",
    "    \n",
    "    if isinstance(last_message, AIMessage) and not (hasattr(last_message, 'tool_calls') and last_message.tool_calls):\n",
    "        if has_tool_messages:\n",
    "            answer = last_message.content\n",
    "            state[\"final_answer\"] = answer\n",
    "            intent_history.append(intent)\n",
    "            state[\"intent_history\"] = intent_history\n",
    "            \n",
    "            if intent == \"DOC_QA\":\n",
    "                for msg in messages:\n",
    "                    if isinstance(msg, ToolMessage) and msg.name == \"document_search\":\n",
    "                        state[\"context\"] = msg.content\n",
    "                        break\n",
    "            \n",
    "            return state\n",
    "    \n",
    "    tool_messages = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, ToolMessage):\n",
    "            tool_messages.append(f\"{msg.name}: {msg.content}\")\n",
    "    \n",
    "    if intent == \"SUMMARY\":\n",
    "        prompt = f\"\"\"이전 대화 내용을 요약해주세요.\n",
    "\n",
    "도구 실행 결과:\n",
    "{chr(10).join(tool_messages) if tool_messages else \"없음\"}\n",
    "\n",
    "사용자 요청: {query}\n",
    "\n",
    "위 정보를 종합하여 사용자의 요청에 맞게 요약해주세요:\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"다음 정보를 바탕으로 사용자 질문에 답변하세요.\n",
    "\n",
    "사용자 질문: {query}\n",
    "\n",
    "도구 실행 결과:\n",
    "{chr(10).join(tool_messages) if tool_messages else \"없음\"}\n",
    "\n",
    "문서 컨텍스트:\n",
    "{context if context else \"없음\"}\n",
    "\n",
    "위 정보를 종합하여 사용자 질문에 정확하고 상세하게 답변하세요:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        answer = response.content if hasattr(response, 'content') else str(response)\n",
    "    except Exception as e:\n",
    "        answer = f\"답변 생성 중 오류 발생: {e}\"\n",
    "    \n",
    "    messages.append(AIMessage(content=answer))\n",
    "    state[\"messages\"] = messages\n",
    "    state[\"final_answer\"] = answer\n",
    "    \n",
    "    intent_history.append(intent)\n",
    "    state[\"intent_history\"] = intent_history\n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8683cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래프 구성 완료\n"
     ]
    }
   ],
   "source": [
    "def router_edge_fn(state: AgentState) -> Literal[\"doc_qa\", \"summary\", \"small_talk\"]:\n",
    "    intent = state.get(\"intent\", \"DOC_QA\")\n",
    "    \n",
    "    if intent == \"SMALL_TALK\":\n",
    "        return \"small_talk\"\n",
    "    elif intent == \"SUMMARY\":\n",
    "        return \"summary\"\n",
    "    else:\n",
    "        return \"doc_qa\"\n",
    "\n",
    "def should_use_tools_from_small_talk(state: AgentState) -> Literal[\"model\", \"end\"]:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return \"end\"\n",
    "    \n",
    "    # 날씨 질문인지 확인\n",
    "    query = state.get(\"query\", \"\")\n",
    "    weather_keywords = [\"날씨\", \"기온\", \"온도\", \"비\", \"눈\", \"맑음\", \"흐림\", \"바람\", \"습도\", \"강수\"]\n",
    "    is_weather_query = any(keyword in query for keyword in weather_keywords)\n",
    "    \n",
    "    # 날씨 질문이면 model 노드로 가서 tool 호출\n",
    "    if is_weather_query and model_with_tools:\n",
    "        return \"model\"\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    if isinstance(last_message, AIMessage) and hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"model\"\n",
    "    \n",
    "    return \"end\"\n",
    "\n",
    "def build_graph():\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    tool_node = ToolNode(tools)\n",
    "    \n",
    "    workflow.add_node(\"classify_intent\", classify_intent)\n",
    "    workflow.add_node(\"doc_qa\", doc_qa_node)\n",
    "    workflow.add_node(\"summary\", summary_node)\n",
    "    workflow.add_node(\"model\", model_node)\n",
    "    workflow.add_node(\"tools\", tool_node)\n",
    "    workflow.add_node(\"check_document_result\", check_document_result_node)\n",
    "    workflow.add_node(\"generate_answer\", generate_answer_node)\n",
    "    workflow.add_node(\"small_talk\", small_talk_node)\n",
    "    \n",
    "    workflow.set_entry_point(\"classify_intent\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"classify_intent\",\n",
    "        router_edge_fn,\n",
    "        {\n",
    "            \"doc_qa\": \"doc_qa\",\n",
    "            \"summary\": \"summary\",\n",
    "            \"small_talk\": \"small_talk\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"doc_qa\", \"check_document_result\")\n",
    "    workflow.add_edge(\"summary\", \"model\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"model\",\n",
    "        should_continue_tools,\n",
    "        {\n",
    "            \"tools\": \"tools\",\n",
    "            \"check_document\": \"check_document_result\",\n",
    "            \"continue\": \"generate_answer\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # tools 실행 후 intent에 따라 라우팅\n",
    "    def tools_router(state: AgentState) -> Literal[\"check_document_result\", \"model\"]:\n",
    "        intent = state.get(\"intent\", \"\")\n",
    "        # DOC_QA일 때만 check_document_result로, 그 외에는 model로\n",
    "        if intent == \"DOC_QA\":\n",
    "            return \"check_document_result\"\n",
    "        return \"model\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"tools\",\n",
    "        tools_router,\n",
    "        {\n",
    "            \"check_document_result\": \"check_document_result\",\n",
    "            \"model\": \"model\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # check_document_result에서 충분하면 generate_answer로, 부족하면 model로\n",
    "    # 이미 확인했고 부족하다고 판단했다면, 추가 도구를 사용한 후이므로 generate_answer로\n",
    "    def check_document_router(state: AgentState) -> Literal[\"model\", \"generate_answer\"]:\n",
    "        is_sufficient = state.get(\"document_sufficient\", False)\n",
    "        document_checked = state.get(\"document_checked\", False)\n",
    "        \n",
    "        if is_sufficient:\n",
    "            return \"generate_answer\"\n",
    "        \n",
    "        # 이미 확인했고 부족하다고 판단했다면, 추가 도구를 사용한 후이므로 generate_answer로\n",
    "        if document_checked:\n",
    "            return \"generate_answer\"\n",
    "        \n",
    "        return \"model\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"check_document_result\",\n",
    "        check_document_router,\n",
    "        {\n",
    "            \"model\": \"model\",\n",
    "            \"generate_answer\": \"generate_answer\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"generate_answer\", END)\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"small_talk\",\n",
    "        should_use_tools_from_small_talk,\n",
    "        {\n",
    "            \"model\": \"model\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    memory = MemorySaver()\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "app = build_graph()\n",
    "print(\"그래프 구성 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066e3694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ASCII 그래프 ===\n",
      "                                  +-----------+                                      \n",
      "                                  | __start__ |                                      \n",
      "                                  +-----------+                                      \n",
      "                                        *                                            \n",
      "                                        *                                            \n",
      "                                        *                                            \n",
      "                               +-----------------+                                   \n",
      "                               | classify_intent |                                   \n",
      "                               +-----------------+.....                              \n",
      "                          .....              ..        .......                       \n",
      "                       ...                     .              .....                  \n",
      "                    ...                         ..                 .......           \n",
      "           +--------+                             .                       ...        \n",
      "           | doc_qa |                             .                         .        \n",
      "           +--------+                             .                         .        \n",
      "                *                                 .                         .        \n",
      "                *                                 .                         .        \n",
      "                *                                 .                         .        \n",
      "    +-----------------------+                +---------+              +------------+ \n",
      "    | check_document_result |                | summary |              | small_talk | \n",
      "    +-----------------------+..              +---------+          ....+------------+ \n",
      "          ..         ..        .......            *          .....          .        \n",
      "        ..             ..             .....       *      ....               .        \n",
      "      ..                 ..                ....   *   ...                   .        \n",
      "     .                     .                  +-------+                     .        \n",
      "     .                     .              ....| model |                     .        \n",
      "     .                     .    ..............+-------+                     .        \n",
      "     .                 .........      ....          .                       .        \n",
      "     .       ..........     .     ....               .                      .        \n",
      "     .  .....               .   ..                   .                      .        \n",
      "+-------+          +-----------------+                .                    ..        \n",
      "| tools |          | generate_answer |                .               .....          \n",
      "+-------+          +-----------------+               .             ...               \n",
      "                                      *****          .        .....                  \n",
      "                                           ***      .      ...                       \n",
      "                                              ***   .   ...                          \n",
      "                                              +---------+                            \n",
      "                                              | __end__ |                            \n",
      "                                              +---------+                            \n",
      "\n",
      "=== Mermaid 다이어그램 ===\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tclassify_intent(classify_intent)\n",
      "\tdoc_qa(doc_qa)\n",
      "\tsummary(summary)\n",
      "\tmodel(model)\n",
      "\ttools(tools)\n",
      "\tcheck_document_result(check_document_result)\n",
      "\tgenerate_answer(generate_answer)\n",
      "\tsmall_talk(small_talk)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> classify_intent;\n",
      "\tcheck_document_result -.-> generate_answer;\n",
      "\tcheck_document_result -.-> model;\n",
      "\tclassify_intent -. \"doc_qa\" .-> doc_qa;\n",
      "\tclassify_intent -. \"small_talk\" .-> small_talk;\n",
      "\tclassify_intent -. \"summary\" .-> summary;\n",
      "\tdoc_qa --> check_document_result;\n",
      "\tmodel -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tmodel -. &nbsp;check_document&nbsp; .-> check_document_result;\n",
      "\tmodel -. &nbsp;continue&nbsp; .-> generate_answer;\n",
      "\tmodel -.-> tools;\n",
      "\tsmall_talk -. &nbsp;end&nbsp; .-> __end__;\n",
      "\tsmall_talk -.-> model;\n",
      "\tsummary --> model;\n",
      "\ttools -.-> check_document_result;\n",
      "\ttools -.-> model;\n",
      "\tgenerate_answer --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "그래프 이미지가 위에 표시되었습니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ASCII 그래프 ===\")\n",
    "app.get_graph().print_ascii()\n",
    "\n",
    "print(\"\\n=== Mermaid 다이어그램 ===\")\n",
    "mermaid_code = app.get_graph().draw_mermaid()\n",
    "\n",
    "import re\n",
    "mermaid_code = re.sub(\n",
    "    r'classify_intent\\s*-\\.->\\s*doc_qa;',\n",
    "    'classify_intent -. \"doc_qa\" .-> doc_qa;',\n",
    "    mermaid_code\n",
    ")\n",
    "mermaid_code = re.sub(\n",
    "    r'classify_intent\\s*-\\.->\\s*summary;',\n",
    "    'classify_intent -. \"summary\" .-> summary;',\n",
    "    mermaid_code\n",
    ")\n",
    "mermaid_code = re.sub(\n",
    "    r'classify_intent\\s*-\\.->\\s*small_talk;',\n",
    "    'classify_intent -. \"small_talk\" .-> small_talk;',\n",
    "    mermaid_code\n",
    ")\n",
    "\n",
    "print(mermaid_code)\n",
    "\n",
    "try:\n",
    "    from IPython.display import Image, display, HTML\n",
    "    import base64\n",
    "    import urllib.parse\n",
    "    import requests\n",
    "    \n",
    "    def mermaid_to_image(mermaid_code: str):\n",
    "        try:\n",
    "            encoded = base64.urlsafe_b64encode(mermaid_code.encode()).decode().rstrip('=')\n",
    "            image_url = f\"https://mermaid.ink/img/{encoded}\"\n",
    "            response = requests.get(image_url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                return Image(data=response.content)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"이미지 생성 오류: {e}\")\n",
    "            return None\n",
    "    \n",
    "    image = mermaid_to_image(mermaid_code)\n",
    "    if image:\n",
    "        display(image)\n",
    "        print(\"\\n그래프 이미지가 위에 표시되었습니다.\")\n",
    "    else:\n",
    "        print(\"\\n온라인 Mermaid 에디터에 다음 코드를 복사하세요:\")\n",
    "        print(\"  https://mermaid.live/\")\n",
    "        print(\"\\n또는 다음 HTML을 브라우저에서 열어보세요:\")\n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js\"></script>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"mermaid\">\n",
    "{mermaid_code}\n",
    "    </div>\n",
    "    <script>\n",
    "        mermaid.initialize({{ startOnLoad: true }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        display(HTML(f'<a href=\"data:text/html;charset=utf-8,{urllib.parse.quote(html_content)}\" download=\"graph.html\">그래프 HTML 다운로드</a>'))\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"\\nIPython이 설치되지 않았습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n이미지 표시 중 오류 발생: {e}\")\n",
    "    print(\"\\n온라인 Mermaid 에디터에 다음 코드를 복사하세요:\")\n",
    "    print(\"  https://mermaid.live/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3922780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[질문] 서울 날씨 알려줘\n",
      "[의도: SMALL_TALK]\n",
      "\n",
      "[사용된 도구] (총 2회)\n",
      "  ✓ weather({'city': '서울'})\n",
      "  ✓ weather({'city': 'Seoul'})\n",
      "\n",
      "[기타 도구 실행 결과]\n",
      "[weather] 오류: 404 Client Error: Not Found for url: http://api.openweathermap.org/data/2.5/weather?q=%EC%84%9C%EC%9A%B8&appid=9ec7874bba26eb7c187fd1d5fbb8d0b0&units=metric&lang=kr...\n",
      "[weather] 도시: Seoul\n",
      "온도: -2.24°C\n",
      "설명: 맑음\n",
      "습도: 64%\n",
      "풍속: 1.54 m/s...\n",
      "\n",
      "============================================================\n",
      "\n",
      "[답변]\n",
      "현재 서울의 날씨는 다음과 같습니다:\n",
      "\n",
      "- **온도:** -2.24°C\n",
      "- **설명:** 맑음\n",
      "- **습도:** 64%\n",
      "- **풍속:** 1.54 m/s\n",
      "\n",
      "추운 날씨에 외출 시 따뜻하게 입으세요!\n",
      "[DEBUG] doc_qa_node: 실행 시작\n",
      "[DEBUG] doc_qa_node: query=애플 주가 알려줘, intent=DOC_QA, messages 개수=7\n",
      "[DEBUG] doc_qa_node: 주가 관련 질문으로 판단 -> stock_price 사용 안내\n",
      "[DEBUG] doc_qa_node: 실행 완료, messages 개수=9\n",
      "[DEBUG] check_document_result_node: 실행 시작\n",
      "[DEBUG] check_document_result_node: query=애플 주가 알려줘, intent=DOC_QA, messages 개수=9\n",
      "[DEBUG] check_document_result_node: 실행 시작\n",
      "[DEBUG] check_document_result_node: query=애플 주가 알려줘, intent=DOC_QA, messages 개수=12\n",
      "\n",
      "[질문] 애플 주가 알려줘\n",
      "[의도: DOC_QA]\n",
      "\n",
      "[사용된 도구] (총 3회)\n",
      "  ✓ weather({'city': '서울'})\n",
      "  ✓ weather({'city': 'Seoul'})\n",
      "  ✓ stock_price({'symbol': 'AAPL'})\n",
      "\n",
      "도구별 호출 횟수: {'weather': 2, 'stock_price': 1}\n",
      "\n",
      "[기타 도구 실행 결과]\n",
      "[weather] 오류: 404 Client Error: Not Found for url: http://api.openweathermap.org/data/2.5/weather?q=%EC%84%9C%EC%9A%B8&appid=9ec7874bba26eb7c187fd1d5fbb8d0b0&units=metric&lang=kr...\n",
      "[weather] 도시: Seoul\n",
      "온도: -2.24°C\n",
      "설명: 맑음\n",
      "습도: 64%\n",
      "풍속: 1.54 m/s...\n",
      "[stock_price] 심볼: AAPL\n",
      "회사명: Apple Inc.\n",
      "현재가(오늘): 273.67\n",
      "전일 종가(어제): 272.19\n",
      "변동액: 1.4800000000000182\n",
      "시가총액: 4061369729024\n",
      "거래량: 144632048\n",
      "통화: USD\n",
      "\n",
      "참고: 변화율 계산이 필요하면 calculator 도구를 사용하여 \"(현재가 - 전일종가) / 전일종가 * 100\" 형식으로 계산하...\n",
      "\n",
      "============================================================\n",
      "\n",
      "[답변]\n",
      "애플(Apple Inc.)의 주가는 현재 273.67 USD입니다. 어제의 종가는 272.19 USD로, 변동액은 1.48 USD입니다. 시가총액은 약 4조 061억 3697만 29024 USD이며, 거래량은 144,632,048입니다.\n",
      "[DEBUG] doc_qa_node: 실행 시작\n",
      "[DEBUG] doc_qa_node: query=테슬라 2025년 3분기 실적 알려줘, intent=DOC_QA, messages 개수=14\n",
      "[DEBUG] doc_qa_node: document_search 실행\n",
      "[DEBUG] doc_qa_node: 검색 쿼리: 테슬라 2025년 3분기 실적 알려줘\n",
      "[DEBUG] doc_qa_node: document_search 결과 길이: 8275자\n",
      "[DEBUG] doc_qa_node: 결과 미리보기: [테이블 1: table_50] Tesla’s Q3 2025 10‑Q reports a provision for income taxes of **$570 million** for the three months ended September 30 2025 (down from **$602 million** YoY) and **$1,098 million** for...\n",
      "[DEBUG] doc_qa_node: 실행 완료, messages 개수=16\n",
      "[DEBUG] check_document_result_node: 실행 시작\n",
      "[DEBUG] check_document_result_node: query=테슬라 2025년 3분기 실적 알려줘, intent=DOC_QA, messages 개수=16\n",
      "[DEBUG] check_document_result_node: document_search state 결과 발견, 길이=8275자\n",
      "[DEBUG] check_document_result_node: 문서 검색 결과 충분성 판단 중...\n",
      "[DEBUG] check_document_result_node: 사용자 질문: 테슬라 2025년 3분기 실적 알려줘\n",
      "[DEBUG] check_document_result_node: 문서 검색 결과 길이: 8275자\n",
      "[DEBUG] check_document_result_node: LLM 판단 결과: 부족: \"제공된 문서 검색 결과에는 테슬라 2025년 3분기 실적의 주요 지표(예: 총 매출, 순이익, 영업 이익 등)와 같은 정보가 포함되어 있지 않아 실적에 대한 전반적인 평가를 할 수 없다.\"\n",
      "[DEBUG] check_document_result_node: 판단: 부족\n",
      "[DEBUG] check_document_result_node: 판단 이유: \"제공된 문서 검색 결과에는 테슬라 2025년 3분기 실적의 주요 지표(예: 총 매출, 순이익, 영업 이익 등)와 같은 정보가 포함되어 있지 않아 실적에 대한 전반적인 평가를 할 수 없다.\"\n",
      "[DEBUG] check_document_result_node: 결과 부족 판단 -> web_search 사용 안내 추가\n",
      "[DEBUG] check_document_result_node: 부족한 이유: \"제공된 문서 검색 결과에는 테슬라 2025년 3분기 실적의 주요 지표(예: 총 매출, 순이익, 영업 이익 등)와 같은 정보가 포함되어 있지 않아 실적에 대한 전반적인 평가를 할 수 없다.\"\n",
      "[DEBUG] check_document_result_node: 실행 시작\n",
      "[DEBUG] check_document_result_node: query=테슬라 2025년 3분기 실적 알려줘, intent=DOC_QA, messages 개수=18\n",
      "[DEBUG] check_document_result_node: document_search state 결과 발견, 길이=8275자\n",
      "[DEBUG] check_document_result_node: 문서 검색 결과 충분성 판단 중...\n",
      "[DEBUG] check_document_result_node: 사용자 질문: 테슬라 2025년 3분기 실적 알려줘\n",
      "[DEBUG] check_document_result_node: 문서 검색 결과 길이: 8275자\n",
      "[DEBUG] check_document_result_node: LLM 판단 결과: 부족: 제공된 정보는 테슬라의 2025년 3분기 세금 관련 데이터(부과세, 세금 효과율)만 포함되어 있으며, 전체적인 실적(매출, 순이익 등)에 대한 정보가 결여되어 있습니다. 따라서 사용자의 질문에 대한 완전한 답변을 제공하지 못합니다.\n",
      "[DEBUG] check_document_result_node: 판단: 부족\n",
      "[DEBUG] check_document_result_node: 판단 이유: 제공된 정보는 테슬라의 2025년 3분기 세금 관련 데이터(부과세, 세금 효과율)만 포함되어 있으며, 전체적인 실적(매출, 순이익 등)에 대한 정보가 결여되어 있습니다. 따라서 사용자의 질문에 대한 완전한 답변을 제공하지 못합니다.\n",
      "[DEBUG] check_document_result_node: 결과 부족 판단 -> web_search 사용 안내 추가\n",
      "[DEBUG] check_document_result_node: 부족한 이유: 제공된 정보는 테슬라의 2025년 3분기 세금 관련 데이터(부과세, 세금 효과율)만 포함되어 있으며, 전체적인 실적(매출, 순이익 등)에 대한 정보가 결여되어 있습니다. 따라서 사용자의 질문에 대한 완전한 답변을 제공하지 못합니다.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_5L9JFmdOXfl71ErapU9V1Rg1, call_i8y79CmX9QaRCFfiT4TyLl7I\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m종료합니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrun_query\u001b[39m\u001b[34m(query, thread_id, show_details)\u001b[39m\n\u001b[32m      2\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: thread_id}}\n\u001b[32m      4\u001b[39m initial_state = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdocument_checked\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     15\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m answer = result.get(\u001b[33m\"\u001b[39m\u001b[33mfinal_answer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mmodel_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(msg, SystemMessage):\n\u001b[32m     75\u001b[39m             filtered_messages.append(msg)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m res = \u001b[43mmodel_with_tools\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [res]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5548\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5541\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5543\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5546\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5547\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5549\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5550\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5551\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1356\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1355\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1359\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1361\u001b[39m ):\n\u001b[32m   1362\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1351\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1344\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1345\u001b[39m             response,\n\u001b[32m   1346\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1347\u001b[39m             metadata=generation_info,\n\u001b[32m   1348\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1349\u001b[39m         )\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1351\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1352\u001b[39m         response = raw_response.parse()\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/study/learning_langchain/.venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_5L9JFmdOXfl71ErapU9V1Rg1, call_i8y79CmX9QaRCFfiT4TyLl7I\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
      "During task with name 'model' and id 'd228f333-c1f0-fd46-a45a-a3f60e476fe6'"
     ]
    }
   ],
   "source": [
    "def run_query(query: str, thread_id: str = \"test-user-1\", show_details: bool = True):\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"messages\": [],\n",
    "        \"intent\": \"\",\n",
    "        \"context\": \"\",\n",
    "        \"iteration_count\": 0,\n",
    "        \"max_iterations\": 3,\n",
    "        \"final_answer\": \"\",\n",
    "        \"intent_history\": [],\n",
    "        \"document_sufficient\": False,\n",
    "        \"document_checked\": False\n",
    "    }\n",
    "    \n",
    "    result = app.invoke(initial_state, config=config)\n",
    "    \n",
    "    answer = result.get(\"final_answer\", \"\")\n",
    "    if not answer:\n",
    "        messages = result.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_msg = messages[-1]\n",
    "            if isinstance(last_msg, AIMessage):\n",
    "                answer = last_msg.content\n",
    "            else:\n",
    "                answer = str(last_msg)\n",
    "        else:\n",
    "            answer = \"답변을 생성할 수 없습니다.\"\n",
    "    \n",
    "    intent = result.get(\"intent\", \"UNKNOWN\")\n",
    "    messages = result.get(\"messages\", [])\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"\\n[질문] {query}\")\n",
    "        print(f\"[의도: {intent}]\")\n",
    "        \n",
    "        tool_calls = []\n",
    "        tool_call_count = {}\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                for tool_call in msg.tool_calls:\n",
    "                    tool_name = tool_call.get('name', 'unknown')\n",
    "                    tool_args = tool_call.get('args', {})\n",
    "                    tool_calls.append(f\"  ✓ {tool_name}({tool_args})\")\n",
    "                    tool_call_count[tool_name] = tool_call_count.get(tool_name, 0) + 1\n",
    "        \n",
    "        for msg in messages:\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                tool_name = msg.name\n",
    "                if tool_name not in tool_call_count:\n",
    "                    tool_calls.append(f\"  ✓ {tool_name}(자동 실행)\")\n",
    "                    tool_call_count[tool_name] = 1\n",
    "        \n",
    "        if result.get(\"context\") and \"document_search\" not in tool_call_count:\n",
    "            tool_calls.append(f\"  ✓ document_search(자동 실행)\")\n",
    "            tool_call_count[\"document_search\"] = 1\n",
    "        \n",
    "        if tool_calls:\n",
    "            print(f\"\\n[사용된 도구] (총 {len(tool_calls)}회)\")\n",
    "            for tool_call in tool_calls:\n",
    "                print(tool_call)\n",
    "            if len(tool_call_count) > 1:\n",
    "                print(f\"\\n도구별 호출 횟수: {dict(tool_call_count)}\")\n",
    "        \n",
    "        document_search_results = []\n",
    "        tool_results = []\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, ToolMessage):\n",
    "                if msg.name == \"document_search\":\n",
    "                    document_search_results.append(msg.content)\n",
    "                else:\n",
    "                    tool_results.append(f\"[{msg.name}] {msg.content[:200]}...\")\n",
    "        \n",
    "        if not document_search_results and result.get(\"context\"):\n",
    "            document_search_results.append(result.get(\"context\"))\n",
    "        \n",
    "        if document_search_results:\n",
    "            print(f\"\\n[문서 검색 결과] (총 {len(document_search_results)}개)\")\n",
    "            for i, doc_result in enumerate(document_search_results, 1):\n",
    "                print(f\"\\n--- 문서 {i} ---\")\n",
    "                print(doc_result[:1000])\n",
    "                if len(doc_result) > 1000:\n",
    "                    print(f\"... (총 {len(doc_result)}자)\")\n",
    "        \n",
    "        if tool_results:\n",
    "            print(f\"\\n[기타 도구 실행 결과]\")\n",
    "            for tool_result in tool_results:\n",
    "                print(tool_result)\n",
    "        \n",
    "        if intent in [\"DOC_QA\", \"SUMMARY\"] and result.get(\"context\"):\n",
    "            print(f\"\\n[참고 문서 컨텍스트]\\n{result['context'][:500]}...\")\n",
    "        \n",
    "        if document_search_results or tool_results or (intent in [\"DOC_QA\", \"SUMMARY\"] and result.get(\"context\")):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n[답변]\\n{answer}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "thread_id = \"test-user-1\"\n",
    "\n",
    "while True:\n",
    "    q = input(\"\\n질문: \").strip()\n",
    "    \n",
    "    if not q:\n",
    "        continue\n",
    "    \n",
    "    if q.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"종료합니다.\")\n",
    "        break\n",
    "    \n",
    "    run_query(q, thread_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf42c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ebe54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
